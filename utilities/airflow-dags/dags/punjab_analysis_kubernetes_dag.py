"""
Punjab Data Analysis Pipeline - Kubernetes Version
This version uses KubernetesPodOperator for scalable, distributed processing
Simplified to process selected tenants dynamically in single tasks
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from kubernetes.client import models as k8s
import logging
import os

# Configure logging
logger = logging.getLogger(__name__)

# Default arguments
default_args = {
    'owner': 'punjab-analytics-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DAG Definition
dag = DAG(
    'punjab_analysis_kubernetes_pipeline',
    default_args=default_args,
    description='Punjab property tax analysis using Kubernetes - Scalable and distributed processing',
    schedule_interval=None,  # Manual triggering only
    catchup=False,
    max_active_tasks=10,
    tags=['punjab', 'kubernetes', 'property-tax', 'scalable'],
    params={
        "tenant_ids": ["pb.adampur", "pb.samana", "pb.amloh"],  # Default tenant list
    },
)

# Configuration
DEFAULT_TENANT_IDS = ['pb.adampur', 'pb.samana', 'pb.amloh']

# Docker image - can be overridden via Airflow Variable or environment variable
# Priority: Airflow Variable > Environment Variable > Default
try:
    DOCKER_IMAGE = Variable.get("punjab_analysis_image", default_var=None)
    logger.info(f"Using Docker image from Airflow Variable: {DOCKER_IMAGE}")
except:
    DOCKER_IMAGE = None

if not DOCKER_IMAGE:
    DOCKER_IMAGE = os.getenv("PUNJAB_DOCKER_IMAGE", "egovio/punjab-analysis:latest")
    logger.info(f"Using Docker image from environment/default: {DOCKER_IMAGE}")

# Namespace - can be overridden via Airflow Variable or environment variable
# Priority: Airflow Variable > Environment Variable > Default
try:
    NAMESPACE = Variable.get("punjab_analysis_namespace", default_var=None)
    logger.info(f"Using namespace from Airflow Variable: {NAMESPACE}")
except:
    NAMESPACE = None

if not NAMESPACE:
    NAMESPACE = os.getenv("AIRFLOW__KUBERNETES__NAMESPACE", "egov")
    logger.info(f"Using namespace from environment/default: {NAMESPACE}")

# Kubernetes Resource Configuration
def get_container_resources(memory_request="512Mi", memory_limit="2Gi", cpu_request="500m", cpu_limit="2000m"):
    """Get standard resource configuration for pods"""
    return k8s.V1ResourceRequirements(
        requests={"memory": memory_request, "cpu": cpu_request},
        limits={"memory": memory_limit, "cpu": cpu_limit}
    )

# Volume Configuration
def get_volume_mounts():
    """Get standard volume mounts for Punjab analysis pods"""
    return [
        k8s.V1VolumeMount(
            name="data-storage",
            mount_path="/data",
        ),
        k8s.V1VolumeMount(
            name="output-storage",
            mount_path="/output",
        ),
        k8s.V1VolumeMount(
            name="secrets-storage",
            mount_path="/app/secrets",
            read_only=True,
        ),
    ]

def get_volumes():
    """Get standard volumes for Punjab analysis pods"""
    return [
        k8s.V1Volume(
            name="data-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-data-pvc"
            ),
        ),
        k8s.V1Volume(
            name="output-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-output-pvc"
            ),
        ),
        k8s.V1Volume(
            name="secrets-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-secrets-pvc"
            ),
        ),
    ]

def get_tenant_ids(**context):
    """Get tenant IDs from DAG run configuration, params, or use defaults with validation"""
    dag_run = context.get('dag_run')
    tenant_ids = None

    # Try to get from DAG run conf first (command line triggers)
    if dag_run and dag_run.conf:
        # Support both 'tenant_ids' (list) and 'tenant_id' (single string)
        if 'tenant_ids' in dag_run.conf:
            tenant_ids = dag_run.conf.get('tenant_ids')
            logger.info(f"Using tenant IDs from DAG run configuration: {tenant_ids}")
        elif 'tenant_id' in dag_run.conf:
            tenant_ids = [dag_run.conf.get('tenant_id')]
            logger.info(f"Using single tenant ID from DAG run configuration: {tenant_ids}")

    # Try to get from DAG params (UI triggers)
    if tenant_ids is None and 'params' in context:
        if 'tenant_ids' in context['params']:
            tenant_ids = context['params']['tenant_ids']
            logger.info(f"Using tenant IDs from DAG params: {tenant_ids}")
        elif 'tenant_id' in context['params']:
            tenant_ids = [context['params']['tenant_id']]
            logger.info(f"Using single tenant ID from DAG params: {tenant_ids}")

    # Use defaults if nothing provided
    if tenant_ids is None:
        tenant_ids = DEFAULT_TENANT_IDS
        logger.info(f"No configuration provided, using default tenant IDs: {tenant_ids}")

    # Validate tenant IDs format
    if not isinstance(tenant_ids, list):
        raise ValueError(f"tenant_ids must be a list, got: {type(tenant_ids)}")

    # VALIDATION: Simple format validation only
    # Data existence will be checked during extraction phase
    logger.info("Validating tenant ID format...")

    for tenant_id in tenant_ids:
        # Check basic format
        if not tenant_id or not isinstance(tenant_id, str):
            raise ValueError(f"Invalid tenant ID: {tenant_id}. Must be a non-empty string.")

        # Must start with 'pb.'
        if not tenant_id.startswith('pb.'):
            raise ValueError(
                f"Invalid tenant format: '{tenant_id}'. Must start with 'pb.'\n"
                f"Example: pb.adampur, pb.samana, pb.amloh"
            )

        # Must have format pb.{name} (exactly 2 parts)
        parts = tenant_id.split('.')
        if len(parts) != 2 or not parts[1]:
            raise ValueError(
                f"Invalid tenant format: '{tenant_id}'. Expected format: 'pb.{{tenant_name}}'\n"
                f"Example: pb.adampur"
            )

    logger.info(f"‚úÖ Tenant format validation passed for: {tenant_ids}")
    logger.info("‚ÑπÔ∏è  Data existence will be checked during extraction phase")

    # Store tenant IDs in XCom for other tasks to use
    context['ti'].xcom_push(key='tenant_ids', value=tenant_ids)

    logger.info("=" * 60)
    logger.info(f"üìã Selected tenants for processing: {', '.join(tenant_ids)}")
    logger.info("=" * 60)

    return tenant_ids

# Start task - gets and validates tenant configuration
start_task = PythonOperator(
    task_id='start_pipeline',
    python_callable=get_tenant_ids,
    provide_context=True,
    dag=dag,
)

# Data Extraction Task - processes all selected tenants
extract_task = KubernetesPodOperator(
    task_id='extract_data',
    name='extract-all-tenants',
    namespace=NAMESPACE,
    image=DOCKER_IMAGE,
    cmds=["python", "process_all_tenants.py"],
    arguments=[],

    # Environment variables for extraction
    # Using list format to support both simple values and ConfigMap/Secret references
    env_vars=[
        # Simple string values
        k8s.V1EnvVar(name='MODE', value='extract'),
        k8s.V1EnvVar(name='TENANT_IDS', value='{{ ti.xcom_pull(task_ids="start_pipeline", key="tenant_ids") | tojson }}'),
        k8s.V1EnvVar(name='OUTPUT_DIR', value='/data'),
        k8s.V1EnvVar(name='EXECUTION_DATE', value='{{ ds }}'),
        k8s.V1EnvVar(name='ENVIRONMENT', value='production'),
        k8s.V1EnvVar(name='DEBUG_MODE', value='false'),
        k8s.V1EnvVar(name='DB_PORT', value='5432'),
        # Database configuration from ConfigMap (egov-config)
        k8s.V1EnvVar(
            name='DB_HOST',
            value_from=k8s.V1EnvVarSource(
                config_map_key_ref=k8s.V1ConfigMapKeySelector(
                    name='egov-config',
                    key='db-host'
                )
            )
        ),
        k8s.V1EnvVar(
            name='DB_NAME',
            value_from=k8s.V1EnvVarSource(
                config_map_key_ref=k8s.V1ConfigMapKeySelector(
                    name='egov-config',
                    key='db-name'
                )
            )
        ),
        # Database credentials from Secret (to be created)
        k8s.V1EnvVar(
            name='DB_USER',
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name='db-credentials',
                    key='username'
                )
            )
        ),
        k8s.V1EnvVar(
            name='DB_PASSWORD',
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name='db-credentials',
                    key='password'
                )
            )
        ),
    ],

    # Resource configuration for extraction (lighter workload)
    container_resources=get_container_resources(
        memory_request="1Gi",
        memory_limit="4Gi",
        cpu_request="1000m",
        cpu_limit="4000m"
    ),

    # Volume mounts
    volume_mounts=get_volume_mounts(),
    volumes=get_volumes(),

    # Kubernetes configuration
    service_account_name="airflow-worker",
    is_delete_operator_pod=True,
    get_logs=True,
    log_events_on_failure=True,
    image_pull_policy="Always",  # Pull image from registry

    # Pod security and execution settings
    security_context=k8s.V1SecurityContext(
        run_as_user=1000,
        run_as_group=1000,
        run_as_non_root=True,
    ),

    dag=dag,
)

# Data Analysis Task - processes all selected tenants
analyze_task = KubernetesPodOperator(
    task_id='analyze_data',
    name='analyze-all-tenants',
    namespace=NAMESPACE,
    image=DOCKER_IMAGE,
    cmds=["python", "process_all_tenants.py"],
    arguments=[],

    # Environment variables for analysis
    env_vars=[
        k8s.V1EnvVar(name='MODE', value='analyze'),
        k8s.V1EnvVar(name='TENANT_IDS', value='{{ ti.xcom_pull(task_ids="start_pipeline", key="tenant_ids") | tojson }}'),
        k8s.V1EnvVar(name='DATA_DIR', value='/data'),
        k8s.V1EnvVar(name='OUTPUT_DIR', value='/output'),
        k8s.V1EnvVar(name='EXECUTION_DATE', value='{{ ds }}'),
        k8s.V1EnvVar(name='ENVIRONMENT', value='production'),
        k8s.V1EnvVar(name='DEBUG_MODE', value='false'),
        k8s.V1EnvVar(name='DB_PORT', value='5432'),
        # Database configuration from ConfigMap
        k8s.V1EnvVar(
            name='DB_HOST',
            value_from=k8s.V1EnvVarSource(
                config_map_key_ref=k8s.V1ConfigMapKeySelector(name='egov-config', key='db-host')
            )
        ),
        k8s.V1EnvVar(
            name='DB_NAME',
            value_from=k8s.V1EnvVarSource(
                config_map_key_ref=k8s.V1ConfigMapKeySelector(name='egov-config', key='db-name')
            )
        ),
        # Database credentials from Secret
        k8s.V1EnvVar(
            name='DB_USER',
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(name='db-credentials', key='username')
            )
        ),
        k8s.V1EnvVar(
            name='DB_PASSWORD',
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(name='db-credentials', key='password')
            )
        ),
    ],

    # Resource configuration for analysis (heavier workload)
    container_resources=get_container_resources(
        memory_request="2Gi",
        memory_limit="8Gi",
        cpu_request="2000m",
        cpu_limit="8000m"
    ),

    # Volume mounts
    volume_mounts=get_volume_mounts(),
    volumes=get_volumes(),

    # Kubernetes configuration
    service_account_name="airflow-worker",
    is_delete_operator_pod=True,
    get_logs=True,
    log_events_on_failure=True,
    image_pull_policy="Always",  # Pull image from registry

    # Pod security and execution settings
    security_context=k8s.V1SecurityContext(
        run_as_user=1000,
        run_as_group=1000,
        run_as_non_root=True,
    ),

    dag=dag,
)

# Upload to Filestore Task - uploads analysis results
upload_task = KubernetesPodOperator(
    task_id='upload_to_filestore',
    name='upload-to-filestore',
    namespace=NAMESPACE,
    image=DOCKER_IMAGE,
    cmds=["python", "upload_to_filestore.py"],
    arguments=[],

    # Environment variables for filestore upload
    env_vars=[
        k8s.V1EnvVar(name='OUTPUT_DIR', value='/output'),
        k8s.V1EnvVar(name='TENANT_IDS', value='{{ ti.xcom_pull(task_ids="start_pipeline", key="tenant_ids") | tojson }}'),
        k8s.V1EnvVar(name='EXECUTION_DATE', value='{{ ds }}'),
        k8s.V1EnvVar(name='ENVIRONMENT', value='production'),

        # Filestore configuration
        k8s.V1EnvVar(name='FILESTORE_ENABLED', value='true'),
        k8s.V1EnvVar(name='FILESTORE_TYPE', value='http'),
        # Filestore URL from egov-service-host ConfigMap
        k8s.V1EnvVar(
            name='FILESTORE_URL',
            value_from=k8s.V1EnvVarSource(
                config_map_key_ref=k8s.V1ConfigMapKeySelector(
                    name='egov-service-host',
                    key='egov-filestore'
                )
            )
        ),
        k8s.V1EnvVar(name='FILESTORE_AUTH_TOKEN', value=''),
        k8s.V1EnvVar(name='FILESTORE_TENANT_ID', value='pb'),
        k8s.V1EnvVar(name='FILESTORE_MODULE', value='punjab-analysis'),
    ],

    # Resource configuration for upload (light workload)
    container_resources=get_container_resources(
        memory_request="512Mi",
        memory_limit="2Gi",
        cpu_request="500m",
        cpu_limit="2000m"
    ),

    # Volume mounts - need output and secrets
    volume_mounts=[
        k8s.V1VolumeMount(
            name="output-storage",
            mount_path="/output",
            read_only=True,  # Read-only since we're only uploading
        ),
        k8s.V1VolumeMount(
            name="secrets-storage",
            mount_path="/app/secrets",
            read_only=True,
        ),
    ],
    volumes=[
        k8s.V1Volume(
            name="output-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-output-pvc"
            ),
        ),
        k8s.V1Volume(
            name="secrets-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-secrets-pvc"
            ),
        ),
    ],

    # Kubernetes configuration
    service_account_name="airflow-worker",
    is_delete_operator_pod=True,
    get_logs=True,
    log_events_on_failure=True,
    image_pull_policy="Always",  # Pull image from registry

    # Pod security and execution settings
    security_context=k8s.V1SecurityContext(
        run_as_user=1000,
        run_as_group=1000,
        run_as_non_root=True,
    ),

    dag=dag,
)

# Completion task
complete_task = PythonOperator(
    task_id='pipeline_complete',
    python_callable=lambda **context: logger.info(
        f"üéâ Punjab Analysis Kubernetes Pipeline completed! "
        f"Execution date: {context['ds']}, Run ID: {context['run_id']}"
    ),
    provide_context=True,
    dag=dag,
)

# Cleanup task using KubernetesPodOperator
cleanup_task = KubernetesPodOperator(
    task_id='cleanup_processed_data',
    name='cleanup-data',
    namespace=NAMESPACE,
    image=DOCKER_IMAGE,
    cmds=["python", "cleanup_data.py"],
    arguments=[],

    # Environment variables for cleanup
    env_vars=[
        k8s.V1EnvVar(name='DATA_DIR', value='/data'),
        k8s.V1EnvVar(name='TENANT_IDS', value='{{ ti.xcom_pull(task_ids="start_pipeline", key="tenant_ids") | tojson }}'),
        k8s.V1EnvVar(name='CLEANUP_MODE', value='immediate'),  # Clean current run data
        k8s.V1EnvVar(name='MAX_AGE_HOURS', value='0'),  # Clean immediately, not after 24 hours
        k8s.V1EnvVar(name='DRY_RUN', value='false'),
        k8s.V1EnvVar(name='ENVIRONMENT', value='production'),
        k8s.V1EnvVar(name='EXECUTION_DATE', value='{{ ds }}'),
    ],

    # Resource configuration for cleanup (light workload)
    container_resources=get_container_resources(
        memory_request="256Mi",
        memory_limit="512Mi",
        cpu_request="100m",
        cpu_limit="500m"
    ),

    # Volume mounts (only need data and secrets)
    volume_mounts=[
        k8s.V1VolumeMount(
            name="data-storage",
            mount_path="/data",
        ),
        k8s.V1VolumeMount(
            name="secrets-storage",
            mount_path="/app/secrets",
            read_only=True,
        ),
    ],
    volumes=[
        k8s.V1Volume(
            name="data-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-data-pvc"
            ),
        ),
        k8s.V1Volume(
            name="secrets-storage",
            persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(
                claim_name="punjab-secrets-pvc"
            ),
        ),
    ],

    # Kubernetes configuration
    service_account_name="airflow-worker",
    is_delete_operator_pod=True,
    get_logs=True,
    log_events_on_failure=True,
    image_pull_policy="Always",  # Pull image from registry

    # Pod security and execution settings
    security_context=k8s.V1SecurityContext(
        run_as_user=1000,
        run_as_group=1000,
        run_as_non_root=True,
    ),

    dag=dag,
)

# Set up linear dependency chain
# Analysis outputs are uploaded to Filestore, then pipeline completes, then cleanup
start_task >> extract_task >> analyze_task >> upload_task >> complete_task >> cleanup_task
